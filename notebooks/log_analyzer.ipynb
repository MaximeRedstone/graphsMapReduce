{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_json(\"../cluster_logs/eventlogs/2222592368926707400/eventlog.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DBCEventLoggingListenerMetadata', 'SparkListenerExecutorAdded',\n",
       "       'SparkListenerBlockManagerAdded', 'SparkListenerEnvironmentUpdate',\n",
       "       'SparkListenerApplicationStart', 'SparkListenerJobStart',\n",
       "       'SparkListenerStageSubmitted', 'SparkListenerTaskStart',\n",
       "       'SparkListenerTaskEnd', 'SparkListenerStageCompleted',\n",
       "       'SparkListenerTaskGettingResult', 'SparkListenerJobEnd'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df['Event'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(event_types, log_df):\n",
    "    \n",
    "    df_list = []\n",
    "    for event_type in event_types: \n",
    "        df = log_df[log_df['Event'] == event_type]\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "event_types = ['SparkListenerJobStart', 'SparkListenerJobEnd']\n",
    "job_list_df = filter_df(event_types, log_df)\n",
    "job_df = job_list_df[0].merge(job_list_df[1], on=['Job ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_job_df(job_df):\n",
    "    \n",
    "    job_df['Duration'] = (job_df['Completion Time'] - job_df['Submission Time']) / 1000\n",
    "    job_df['Job ID'] = job_df['Job ID'].astype(int)\n",
    "    job_df.set_index(['Job ID'], inplace=True)\n",
    "    \n",
    "    stage_df_list = []\n",
    "    for index, row in job_df.iterrows():\n",
    "        tmp_df = row['Stage Infos']\n",
    "        tmp_df = pd.DataFrame(tmp_df)\n",
    "        tmp_df['Job ID'] = index\n",
    "        stage_df_list.append(tmp_df)\n",
    "    \n",
    "    stage_df = pd.concat(stage_df_list)\n",
    "    stage_df.set_index(['Stage ID'], inplace=True)\n",
    "    stage_df = stage_df[['Job ID']]\n",
    "    return job_df, stage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df, stage_df = process_job_df(job_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_x</th>\n",
       "      <th>Submission Time</th>\n",
       "      <th>Stage Infos</th>\n",
       "      <th>Stage IDs</th>\n",
       "      <th>Properties</th>\n",
       "      <th>Event_y</th>\n",
       "      <th>Completion Time</th>\n",
       "      <th>Job Result</th>\n",
       "      <th>Duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Job ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SparkListenerJobStart</td>\n",
       "      <td>1.614261e+12</td>\n",
       "      <td>[{'Stage ID': 0, 'Stage Attempt ID': 0, 'Stage...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>{'spark.scheduler.pool': '3563561430952536780'...</td>\n",
       "      <td>SparkListenerJobEnd</td>\n",
       "      <td>1.614261e+12</td>\n",
       "      <td>{'Result': 'JobSucceeded'}</td>\n",
       "      <td>42.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SparkListenerJobStart</td>\n",
       "      <td>1.614261e+12</td>\n",
       "      <td>[{'Stage ID': 5, 'Stage Attempt ID': 0, 'Stage...</td>\n",
       "      <td>[5, 6, 3, 7, 4]</td>\n",
       "      <td>{'spark.scheduler.pool': '3563561430952536780'...</td>\n",
       "      <td>SparkListenerJobEnd</td>\n",
       "      <td>1.614261e+12</td>\n",
       "      <td>{'Result': 'JobSucceeded'}</td>\n",
       "      <td>58.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SparkListenerJobStart</td>\n",
       "      <td>1.614261e+12</td>\n",
       "      <td>[{'Stage ID': 12, 'Stage Attempt ID': 0, 'Stag...</td>\n",
       "      <td>[12, 9, 13, 10, 14, 11, 8]</td>\n",
       "      <td>{'spark.scheduler.pool': '3563561430952536780'...</td>\n",
       "      <td>SparkListenerJobEnd</td>\n",
       "      <td>1.614261e+12</td>\n",
       "      <td>{'Result': 'JobSucceeded'}</td>\n",
       "      <td>161.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SparkListenerJobStart</td>\n",
       "      <td>1.614261e+12</td>\n",
       "      <td>[{'Stage ID': 15, 'Stage Attempt ID': 0, 'Stag...</td>\n",
       "      <td>[15, 19, 16, 20, 17, 21, 18, 22, 23]</td>\n",
       "      <td>{'spark.scheduler.pool': '3563561430952536780'...</td>\n",
       "      <td>SparkListenerJobEnd</td>\n",
       "      <td>1.614261e+12</td>\n",
       "      <td>{'Result': 'JobSucceeded'}</td>\n",
       "      <td>54.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SparkListenerJobStart</td>\n",
       "      <td>1.614261e+12</td>\n",
       "      <td>[{'Stage ID': 33, 'Stage Attempt ID': 0, 'Stag...</td>\n",
       "      <td>[33, 30, 27, 34, 31, 32, 24, 28, 25, 29, 26]</td>\n",
       "      <td>{'spark.scheduler.pool': '3563561430952536780'...</td>\n",
       "      <td>SparkListenerJobEnd</td>\n",
       "      <td>1.614261e+12</td>\n",
       "      <td>{'Result': 'JobSucceeded'}</td>\n",
       "      <td>9.891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Event_x  Submission Time  \\\n",
       "Job ID                                           \n",
       "0       SparkListenerJobStart     1.614261e+12   \n",
       "1       SparkListenerJobStart     1.614261e+12   \n",
       "2       SparkListenerJobStart     1.614261e+12   \n",
       "3       SparkListenerJobStart     1.614261e+12   \n",
       "4       SparkListenerJobStart     1.614261e+12   \n",
       "\n",
       "                                              Stage Infos  \\\n",
       "Job ID                                                      \n",
       "0       [{'Stage ID': 0, 'Stage Attempt ID': 0, 'Stage...   \n",
       "1       [{'Stage ID': 5, 'Stage Attempt ID': 0, 'Stage...   \n",
       "2       [{'Stage ID': 12, 'Stage Attempt ID': 0, 'Stag...   \n",
       "3       [{'Stage ID': 15, 'Stage Attempt ID': 0, 'Stag...   \n",
       "4       [{'Stage ID': 33, 'Stage Attempt ID': 0, 'Stag...   \n",
       "\n",
       "                                           Stage IDs  \\\n",
       "Job ID                                                 \n",
       "0                                          [0, 1, 2]   \n",
       "1                                    [5, 6, 3, 7, 4]   \n",
       "2                         [12, 9, 13, 10, 14, 11, 8]   \n",
       "3               [15, 19, 16, 20, 17, 21, 18, 22, 23]   \n",
       "4       [33, 30, 27, 34, 31, 32, 24, 28, 25, 29, 26]   \n",
       "\n",
       "                                               Properties  \\\n",
       "Job ID                                                      \n",
       "0       {'spark.scheduler.pool': '3563561430952536780'...   \n",
       "1       {'spark.scheduler.pool': '3563561430952536780'...   \n",
       "2       {'spark.scheduler.pool': '3563561430952536780'...   \n",
       "3       {'spark.scheduler.pool': '3563561430952536780'...   \n",
       "4       {'spark.scheduler.pool': '3563561430952536780'...   \n",
       "\n",
       "                    Event_y  Completion Time                  Job Result  \\\n",
       "Job ID                                                                     \n",
       "0       SparkListenerJobEnd     1.614261e+12  {'Result': 'JobSucceeded'}   \n",
       "1       SparkListenerJobEnd     1.614261e+12  {'Result': 'JobSucceeded'}   \n",
       "2       SparkListenerJobEnd     1.614261e+12  {'Result': 'JobSucceeded'}   \n",
       "3       SparkListenerJobEnd     1.614261e+12  {'Result': 'JobSucceeded'}   \n",
       "4       SparkListenerJobEnd     1.614261e+12  {'Result': 'JobSucceeded'}   \n",
       "\n",
       "        Duration  \n",
       "Job ID            \n",
       "0         42.414  \n",
       "1         58.101  \n",
       "2        161.005  \n",
       "3         54.654  \n",
       "4          9.891  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stage_df(stage_list_df):\n",
    "    \n",
    "    ret_list_info = []\n",
    "    ret_list_rdd = []\n",
    "    rdd_info_list = []\n",
    "    for stage_df in stage_list_df:\n",
    "                \n",
    "        info_df_list = []\n",
    "        for index, row in stage_df.iterrows():\n",
    "            \n",
    "            tmp_df = row['Stage Info']\n",
    "                        \n",
    "            rdd_info = tmp_df['RDD Info']\n",
    "#             print(rdd_info)\n",
    "            rdd_info_df = pd.DataFrame(rdd_info)\n",
    "#             print(rdd_info_df)\n",
    "            rdd_info_df['Stage ID'] = tmp_df['Stage ID']\n",
    "            rdd_info_list.append(rdd_info_df)\n",
    "            \n",
    "            tmp_df = pd.DataFrame.from_dict(tmp_df, orient='index')\n",
    "            tmp_df = tmp_df.transpose()\n",
    "            tmp_df.set_index(['Stage ID'], inplace=True)\n",
    "            info_df_list.append(tmp_df)\n",
    "            \n",
    "        info_df = pd.concat(info_df_list)\n",
    "        rdd_info_df_ret = pd.concat(rdd_info_list)\n",
    "        \n",
    "        ret_list_info.append(info_df)\n",
    "        ret_list_rdd.append(rdd_info_df_ret)\n",
    "        \n",
    "    return ret_list_info, ret_list_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Completion Time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Completion Time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4530f25b5e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstage_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_df_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstage_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Duration'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstage_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Completion Time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstage_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Submission Time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrdd_info_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Completion Time'"
     ]
    }
   ],
   "source": [
    "stages = ['SparkListenerStageSubmitted', 'SparkListenerStageCompleted']\n",
    "stage_list_df = filter_df(stages, log_df)\n",
    "# stage_list_df[0]['Stage Info']\n",
    "info_df_list, rdd_info_list = process_stage_df(stage_list_df)\n",
    "\n",
    "stage_df = stage_df.merge(info_df_list[1], left_index=True, right_index=True)\n",
    "stage_df['Duration'] = (stage_df['Completion Time'] - stage_df['Submission Time']) / 1000\n",
    "\n",
    "rdd_info_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task_types = ['SparkListenerTaskStart', 'SparkListenerTaskEnd', 'SparkListenerTaskGettingResult']\n",
    "tasks_df_list = filter_df(task_types, log_df)\n",
    "tasks_df_list[1].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tasks(task_df_list):\n",
    "    \n",
    "    ret_list = []\n",
    "    for task_df in task_df_list:\n",
    "        \n",
    "        info_df_list = []\n",
    "        for index, row in task_df.iterrows():\n",
    "            \n",
    "            tmp_df = row['Task Info']\n",
    "            tmp_df = pd.DataFrame.from_dict(tmp_df, orient='index')\n",
    "            tmp_df = tmp_df.transpose()\n",
    "            tmp_df['Stage ID'] = int(row['Stage ID'])\n",
    "            tmp_df.set_index(['Task ID'], inplace=True)\n",
    "            info_df_list.append(tmp_df)\n",
    "            \n",
    "        info_df = pd.concat(info_df_list)\n",
    "        ret_list.append(info_df)\n",
    "        \n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list = process_tasks(tasks_df_list[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(task_list[0].info())\n",
    "# print(task_list[1].info())\n",
    "task_list[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_infos_0_0 = pd.DataFrame(stage_df_0.loc[0, 'RDD Info'])\n",
    "rdd_infos_0_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = ['SparkListenerStageSubmitted', 'SparkListenerStageCompleted']\n",
    "stage_start_df = log_df[log_df['Event'] == stages[0]]\n",
    "stage_end_df = log_df[log_df['Event'] == stages[1]]\n",
    "stage_result_df = log_df[log_df['Event'] == stages[2]]\n",
    "\n",
    "stage_start_df.dropna(axis=1, how='all', inplace=True)\n",
    "stage_end_df.dropna(axis=1, how='all', inplace=True)\n",
    "stage_result_df.dropna(axis=1, how='all', inplace=True)\n",
    "# task_df = task_start_df.merge(task_end_df, on=['Stage ID'])\n",
    "# task_df\n",
    "stage_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[    \n",
    "    {'RDD ID': 0, 'Name': 'dbfs:/FileStore/graph_10M.txt', 'Scope': '{\"id\":\"0\",\"name\":\"textFile\"}', 'Callsite': 'textFile at NativeMethodAccessorImpl.java:0', 'Parent IDs': [], \n",
    "     'Storage Level':\n",
    "         {'Use Disk': False, 'Use Memory': False, 'Deserialized': False, 'Replication': 1},\n",
    "     'Barrier': False, 'Number of Partitions': 2, 'Number of Cached Partitions': 0, 'Memory Size': 0, 'Disk Size': 0},\n",
    "    \n",
    "    {'RDD ID': 1, 'Name': 'dbfs:/FileStore/graph_10M.txt', 'Scope': '{\"id\":\"0\",\"name\":\"textFile\"}', 'Callsite': 'textFile at NativeMethodAccessorImpl.java:0', 'Parent IDs': [0],\n",
    "     'Storage Level':\n",
    "         {'Use Disk': False, 'Use Memory': False, 'Deserialized': False, 'Replication': 1},\n",
    "     'Barrier': False, 'Number of Partitions': 2, 'Number of Cached Partitions': 0, 'Memory Size': 0, 'Disk Size': 0}\n",
    "    \n",
    "    {'RDD ID': 2, 'Name': 'PythonRDD', 'Callsite': 'groupByKey at <command-193306450925007>:32', 'Parent IDs': [1],\n",
    "     'Storage Level': \n",
    "         {'Use Disk': False, 'Use Memory': False, 'Deserialized': False, 'Replication': 1},\n",
    "     'Barrier': False, 'Number of Partitions': 2, 'Number of Cached Partitions': 0, 'Memory Size': 0, 'Disk Size': 0},\n",
    "    \n",
    "    {'RDD ID': 3, 'Name': 'PairwiseRDD', 'Callsite': 'groupByKey at <command-193306450925007>:32', 'Parent IDs': [2], \n",
    "     'Storage Level': \n",
    "         {'Use Disk': False, 'Use Memory': False, 'Deserialized': False, 'Replication': 1},\n",
    "     'Barrier': False, 'Number of Partitions': 2, 'Number of Cached Partitions': 0, 'Memory Size': 0, 'Disk Size': 0\n",
    "    }, \n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
