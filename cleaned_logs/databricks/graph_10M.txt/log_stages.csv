Stage ID;Job ID;Stage Attempt ID;Stage Name;Number of Tasks;Parent IDs;Details;Submission Time;Completion Time;Failure Reason;Duration
0;0;0;groupByKey at <command-1341553813220844>:112;2;[];"org.apache.spark.rdd.RDD.<init>(RDD.scala:109)
org.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:123)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
py4j.Gateway.invoke(Gateway.java:250)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.GatewayConnection.run(GatewayConnection.java:251)
java.lang.Thread.run(Thread.java:748)";2021-03-10 13:17:30.686;2021-03-10 13:17:38.934;"Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, ip-10-172-243-207.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: 'IndexError: list index out of range', from <command-1341553813220844>, line 102. Full traceback below:
Traceback (most recent call last):
  File ""/databricks/spark/python/pyspark/worker.py"", line 676, in main
    process()
  File ""/databricks/spark/python/pyspark/worker.py"", line 666, in process
    out_iter = func(split_index, iterator)
  File ""/databricks/spark/python/pyspark/rdd.py"", line 2627, in pipeline_func
    return func(split, prev_func(split, iterator))
  File ""/databricks/spark/python/pyspark/rdd.py"", line 2627, in pipeline_func
    return func(split, prev_func(split, iterator))
  File ""/databricks/spark/python/pyspark/rdd.py"", line 425, in func
    return f(iterator)
  File ""/databricks/spark/python/pyspark/rdd.py"", line 2061, in combine
    merger.mergeValues(iterator)
  File ""/databricks/spark/python/pyspark/shuffle.py"", line 238, in mergeValues
    for k, v in iterator:
  File ""/databricks/spark/python/pyspark/util.py"", line 110, in wrapper
    return f(*args, **kwargs)
  File ""<command-1341553813220844>"", line 102, in <lambda>
IndexError: list index out of range

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:603)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:738)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:721)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:556)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)
	at org.apache.spark.scheduler.Task.run(Task.scala:117)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:677)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:680)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:";8.248
