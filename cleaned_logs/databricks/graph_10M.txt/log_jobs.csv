Job ID;Submission Time;Stage IDs;Completion Time;Job Result;Duration;spark.job.interruptOnCancel;spark.databricks.api.url;spark.rdd.scope;callSite.short;spark.rdd.scope.noOverride;spark.job.description;spark.databricks.replId;spark.jobGroup.id;spark.databricks.notebook.id;spark.databricks.isolationID;spark.databricks.inherited.credentials.keys.spark.databricks.api.token;spark.databricks.notebook.path;spark.databricks.token;spark.scheduler.pool;spark.databricks.env;Filename
0;2021-03-10 13:17:30.510;[0, 1, 2];2021-03-10 13:17:38.960;"{'Result': 'JobFailed', 'Exception': {'Message': 'Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, ip-10-172-243-207.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: \'IndexError: list index out of range\', from <command-1341553813220844>, line 102. Full traceback below:\nTraceback (most recent call last):\n  File ""/databricks/spark/python/pyspark/worker.py"", line 676, in main\n    process()\n  File ""/databricks/spark/python/pyspark/worker.py"", line 666, in process\n    out_iter = func(split_index, iterator)\n  File ""/databricks/spark/python/pyspark/rdd.py"", line 2627, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File ""/databricks/spark/python/pyspark/rdd.py"", line 2627, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File ""/databricks/spark/python/pyspark/rdd.py"", line 425, in func\n    return f(iterator)\n  File ""/databricks/spark/python/pyspark/rdd.py"", line 2061, in combine\n    merger.mergeValues(iterator)\n  File ""/databricks/spark/python/pyspark/shuffle.py"", line 238, in mergeValues\n    for k, v in iterator:\n  File ""/databricks/spark/python/pyspark/util.py"", line 110, in wrapper\n    return f(*args, **kwargs)\n  File ""<command-1341553813220844>"", line 102, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:603)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:738)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:721)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:556)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:677)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:', 'Stack Trace': [{'Declaring Class': 'org.apache.spark.scheduler.DAGScheduler', 'Method Name': 'failJobAndIndependentStages', 'File Name': 'DAGScheduler.scala', 'Line Number': 2519}, {'Declaring Class': 'org.apache.spark.scheduler.DAGScheduler', 'Method Name': '$anonfun$abortStage$2', 'File Name': 'DAGScheduler.scala', 'Line Number': 2466}, {'Declaring Class': 'org.apache.spark.scheduler.DAGScheduler', 'Method Name': '$anonfun$abortStage$2$adapted', 'File Name': 'DAGScheduler.scala', 'Line Number': 2460}, {'Declaring Class': 'scala.collection.mutable.ResizableArray', 'Method Name': 'foreach', 'File Name': 'ResizableArray.scala', 'Line Number': 62}, {'Declaring Class': 'scala.collection.mutable.ResizableArray', 'Method Name': 'foreach$', 'File Name': 'ResizableArray.scala', 'Line Number': 55}, {'Declaring Class': 'scala.collection.mutable.ArrayBuffer', 'Method Name': 'foreach', 'File Name': 'ArrayBuffer.scala', 'Line Number': 49}, {'Declaring Class': 'org.apache.spark.scheduler.DAGScheduler', 'Method Name': 'abortStage', 'File Name': 'DAGScheduler.scala', 'Line Number': 2460}, {'Declaring Class': 'org.apache.spark.scheduler.DAGScheduler', 'Method Name': '$anonfun$handleTaskSetFailed$1', 'File Name': 'DAGScheduler.scala', 'Line Number': 1152}, {'Declaring Class': 'org.apache.spark.scheduler.DAGScheduler', 'Method Name': '$anonfun$handleTaskSetFailed$1$adapted', 'File Name': 'DAGScheduler.scala', 'Line Number': 1152}, {'Declaring Class': 'scala.Option', 'Method Name': 'foreach', 'File Name': 'Option.scala', 'Line Number': 407}, {'Declaring Class': 'org.apache.spark.scheduler.DAGScheduler', 'Method Name': 'handleTaskSetFailed', 'File Name': 'DAGScheduler.scala', 'Line Number': 1152}, {'Declaring Class': 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop', 'Method Name': 'doOnReceive', 'File Name': 'DAGScheduler.scala', 'Line Number': 2721}, {'Declaring Class': 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop', 'Method Name': 'onReceive', 'File Name': 'DAGScheduler.scala', 'Line Number': 2668}, {'Declaring Class': 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop', 'Method Name': 'onReceive', 'File Name': 'DAGScheduler.scala', 'Line Number': 2656}, {'Declaring Class': 'org.apache.spark.util.EventLoop$$anon$1', 'Method Name': 'run', 'File Name': 'EventLoop.scala', 'Line Number': 49}, {'Declaring Class': 'org.apache.spark.scheduler.DAGScheduler', 'Method Name': 'runJob', 'File Name': 'DAGScheduler.scala', 'Line Number': 938}, {'Declaring Class': 'org.apache.spark.SparkContext', 'Method Name': 'runJob', 'File Name': 'SparkContext.scala', 'Line Number': 2333}, {'Declaring Class': 'org.apache.spark.SparkContext', 'Method Name': 'runJob', 'File Name': 'SparkContext.scala', 'Line Number': 2354}, {'Declaring Class': 'org.apache.spark.SparkContext', 'Method Name': 'runJob', 'File Name': 'SparkContext.scala', 'Line Number': 2373}, {'Declaring Class': 'org.apache.spark.SparkContext', 'Method Name': 'runJob', 'File Name': 'SparkContext.scala', 'Line Number': 2398}, {'Declaring Class': 'org.apache.spark.rdd.RDD', 'Method Name': '$anonfun$collect$1', 'File Name': 'RDD.scala', 'Line Number': 1011}, {'Declaring Class': 'org.apache.spark.rdd.RDDOperationScope$', 'Method Name': 'withScope', 'File Name': 'RDDOperationScope.scala', 'Line Number': 165}, {'Declaring Class': 'org.apache.spark.rdd.RDDOperationScope$', 'Method Name': 'withScope', 'File Name': 'RDDOperationScope.scala', 'Line Number': 125}, {'Declaring Class': 'org.apache.spark.rdd.RDDOperationScope$', 'Method Name': 'withScope', 'File Name': 'RDDOperationScope.scala', 'Line Number': 112}, {'Declaring Class': 'org.apache.spark.rdd.RDD', 'Method Name': 'withScope', 'File Name': 'RDD.scala', 'Line Number': 395}, {'Declaring Class': 'org.apache.spark.rdd.RDD', 'Method Name': 'collect', 'File Name': 'RDD.scala', 'Line Number': 1010}, {'Declaring Class': 'org.apache.spark.api.python.PythonRDD$', 'Method Name': 'collectAndServe', 'File Name': 'PythonRDD.scala', 'Line Number': 260}, {'Declaring Class': 'org.apache.spark.api.python.PythonRDD', 'Method Name': 'collectAndServe', 'File Name': 'PythonRDD.scala', 'Line Number': -1}, {'Declaring Class': 'sun.reflect.NativeMethodAccessorImpl', 'Method Name': 'invoke0', 'File Name': 'NativeMethodAccessorImpl.java', 'Line Number': -2}, {'Declaring Class': 'sun.reflect.NativeMethodAccessorImpl', 'Method Name': 'invoke', 'File Name': 'NativeMethodAccessorImpl.java', 'Line Number': 62}, {'Declaring Class': 'sun.reflect.DelegatingMethodAccessorImpl', 'Method Name': 'invoke', 'File Name': 'DelegatingMethodAccessorImpl.java', 'Line Number': 43}, {'Declaring Class': 'java.lang.reflect.Method', 'Method Name': 'invoke', 'File Name': 'Method.java', 'Line Number': 498}, {'Declaring Class': 'py4j.reflection.MethodInvoker', 'Method Name': 'invoke', 'File Name': 'MethodInvoker.java', 'Line Number': 244}, {'Declaring Class': 'py4j.reflection.ReflectionEngine', 'Method Name': 'invoke', 'File Name': 'ReflectionEngine.java', 'Line Number': 380}, {'Declaring Class': 'py4j.Gateway', 'Method Name': 'invoke', 'File Name': 'Gateway.java', 'Line Number': 295}, {'Declaring Class': 'py4j.commands.AbstractCommand', 'Method Name': 'invokeMethod', 'File Name': 'AbstractCommand.java', 'Line Number': 132}, {'Declaring Class': 'py4j.commands.CallCommand', 'Method Name': 'execute', 'File Name': 'CallCommand.java', 'Line Number': 79}, {'Declaring Class': 'py4j.GatewayConnection', 'Method Name': 'run', 'File Name': 'GatewayConnection.java', 'Line Number': 251}, {'Declaring Class': 'java.lang.Thread', 'Method Name': 'run', 'File Name': 'Thread.java', 'Line Number': 748}]}}";8.45;true;https://community.cloud.databricks.com;"{""id"":""7"",""name"":""collect""}";collect at <command-1341553813220844>:120;true;"user_inputs = {
  ""10M"": 5,
}

for k, v in user...";ReplId-6ef78-da213-a5c42-5;7996015390673454117_8496327879434620583_9973e02677e14d6c84fffda631df4636;193306450925006;3150c5d3-42b7-4e57-803e-ca24e349e004;*********(redacted);/Users/hugo.ehlinger@polytechnique.edu/Graphs/GraphConnectedComponentsFinder;[REDACTED];;;graph_10M.txt
