OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
/databricks/python/lib/python3.7/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.
  "You should import from traitlets.config instead.", ShimWarning)
Wed Feb 24 16:17:04 2021 py4j imported
Wed Feb 24 16:17:04 2021 Python shell started with PID  1084  and guid  4ea1b590f55d472ea3b88bd7b7fdebb8
Wed Feb 24 16:17:04 2021 Initialized gateway on port 40439
Wed Feb 24 16:17:08 2021 Python shell executor start
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<command-3407598830662274> in <module>
     10   # CCF-Iterate
     11   it_map = input.flatMap(lambda x: ((x[0], x[1]), (x[1], x[0])))
---> 12   it_groupby = it_map.groupByKey().mapValues(lambda x: sorted(x))
     13   it_reduce = it_groupby.flatMap(lambda x: reduce_ccf(x))
     14 

/databricks/spark/python/pyspark/rdd.py in groupByKey(self, numPartitions, partitionFunc)
   2063 
   2064         locally_combined = self.mapPartitions(combine, preservesPartitioning=True)
-> 2065         shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)
   2066 
   2067         def groupByKey(it):

/databricks/spark/python/pyspark/rdd.py in partitionBy(self, numPartitions, partitionFunc)
   1865         """
   1866         if numPartitions is None:
-> 1867             numPartitions = self._defaultReducePartitions()
   1868         partitioner = Partitioner(numPartitions, partitionFunc)
   1869         if self.partitioner == partitioner:

/databricks/spark/python/pyspark/rdd.py in _defaultReducePartitions(self)
   2377             return self.ctx.defaultParallelism
   2378         else:
-> 2379             return self.getNumPartitions()
   2380 
   2381     def lookup(self, key):

/databricks/spark/python/pyspark/rdd.py in getNumPartitions(self)
   2643 
   2644     def getNumPartitions(self):
-> 2645         return self._prev_jrdd.partitions().size()[0m
   2646 
   2647     @property

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-> 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    125     def deco(*a, **kw):
    126         try:
--> 127             return f(*a, **kw)
    128         except py4j.protocol.Py4JJavaError as e:
    129             converted = convert_exception(e.java_exception)

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     "An error occurred while calling {0}{1}{2}.\n".
--> 328                     format(target_id, ".", name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o727.partitions.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: /graphsMapReduce/generator/data/graph_10M.txt
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:213)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:283)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:279)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:283)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:279)
	at org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:62)
	at org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:62)
	at org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
	at py4j.Gateway.invoke(Gateway.java:295)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:251)
	at java.lang.Thread.run(Thread.java:748)
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<command-3407598830662274> in <module>
     10   # CCF-Iterate
     11   it_map = input.flatMap(lambda x: ((x[0], x[1]), (x[1], x[0])))
---> 12   it_groupby = it_map.groupByKey().mapValues(lambda x: sorted(x))
     13   it_reduce = it_groupby.flatMap(lambda x: reduce_ccf(x))
     14 

/databricks/spark/python/pyspark/rdd.py in groupByKey(self, numPartitions, partitionFunc)
   2063 
   2064         locally_combined = self.mapPartitions(combine, preservesPartitioning=True)
-> 2065         shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)
   2066 
   2067         def groupByKey(it):

/databricks/spark/python/pyspark/rdd.py in partitionBy(self, numPartitions, partitionFunc)
   1865         """
   1866         if numPartitions is None:
-> 1867             numPartitions = self._defaultReducePartitions()
   1868         partitioner = Partitioner(numPartitions, partitionFunc)
   1869         if self.partitioner == partitioner:

/databricks/spark/python/pyspark/rdd.py in _defaultReducePartitions(self)
   2377             return self.ctx.defaultParallelism
   2378         else:
-> 2379             return self.getNumPartitions()
   2380 
   2381     def lookup(self, key):

/databricks/spark/python/pyspark/rdd.py in getNumPartitions(self)
   2643 
   2644     def getNumPartitions(self):
-> 2645         return self._prev_jrdd.partitions().size()[0m
   2646 
   2647     @property

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-> 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    125     def deco(*a, **kw):
    126         try:
--> 127             return f(*a, **kw)
    128         except py4j.protocol.Py4JJavaError as e:
    129             converted = convert_exception(e.java_exception)

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     "An error occurred while calling {0}{1}{2}.\n".
--> 328                     format(target_id, ".", name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o773.partitions.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: /graphsMapReduce/generator/data/graph_10M.txt
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:213)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:283)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:279)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:283)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:279)
	at org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:62)
	at org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:62)
	at org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
	at py4j.Gateway.invoke(Gateway.java:295)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:251)
	at java.lang.Thread.run(Thread.java:748)
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<command-193306450925007> in <module>
      2 # text_file = sc.textFile('/FileStore/tables/web_Google.txt')
      3 text_file = sc.textFile('graphsMapReduce/generator/data/graph_10M.txt')
----> 4 text_file.collect()

/databricks/spark/python/pyspark/rdd.py in collect(self)
    901         # Default path used in OSS Spark / for non-credential passthrough clusters:
    902         with SCCallSiteSync(self.context) as css:
--> 903             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    904         return list(_load_from_socket(sock_info, self._jrdd_deserializer))
    905 

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-> 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    125     def deco(*a, **kw):
    126         try:
--> 127             return f(*a, **kw)
    128         except py4j.protocol.Py4JJavaError as e:
    129             converted = convert_exception(e.java_exception)

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     "An error occurred while calling {0}{1}{2}.\n".
--> 328                     format(target_id, ".", name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: /graphsMapReduce/generator/data/graph_10M.txt
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:213)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:283)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:279)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:283)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:279)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:395)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1010)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
	at py4j.Gateway.invoke(Gateway.java:295)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:251)
	at java.lang.Thread.run(Thread.java:748)
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<command-193306450925007> in <module>
      2 # text_file = sc.textFile('/FileStore/tables/web_Google.txt')
      3 text_file = sc.textFile('/databricks/driver/graphsMapReduce/generator/data/graph_10M.txt')
----> 4 text_file.collect()

/databricks/spark/python/pyspark/rdd.py in collect(self)
    901         # Default path used in OSS Spark / for non-credential passthrough clusters:
    902         with SCCallSiteSync(self.context) as css:
--> 903             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    904         return list(_load_from_socket(sock_info, self._jrdd_deserializer))
    905 

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-> 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    125     def deco(*a, **kw):
    126         try:
--> 127             return f(*a, **kw)
    128         except py4j.protocol.Py4JJavaError as e:
    129             converted = convert_exception(e.java_exception)

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     "An error occurred while calling {0}{1}{2}.\n".
--> 328                     format(target_id, ".", name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: /databricks/driver/graphsMapReduce/generator/data/graph_10M.txt
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:213)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:283)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:279)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:283)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:279)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:395)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1010)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
	at py4j.Gateway.invoke(Gateway.java:295)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:251)
	at java.lang.Thread.run(Thread.java:748)
  File "<command-3595702639439416>", line 1
    la -la ../../dbfs
            ^
SyntaxError: invalid syntax
Dropped logging in PythonShell:

b' 588\r\n-r-------- 1 root root 0 Feb 24 16:53 589\r\n-r-------- 1 root root 0 Feb 24 16:53 59\r\n-r-------- 1 root root 0 Feb 24 16:53 590\r\n-r-------- 1 root root 0 Feb 24 16:53 591\r\n-r-------- 1 root root 0 Feb 24 16:53 592\r\n-r-------- 1 root root 0 Feb 24 16:53 593\r\n-r-------- 1 root root 0 Feb 24 16:53 594\r\n-r-------- 1 root root 0 Feb 24 16:53 595\r\n-r-------- 1 root root 0 Feb 24 16:53 596\r\n-r-------- 1 root root 0 Feb 24 16:53 597\r\n-r-------- 1 root root 0 Feb 24 16:53 598\r\n-r-------- 1 root root 0 Feb 24 16:53 599\r\n-r-------- 1 root root 0 Feb 24 16:53 6\r\n-r-------- 1 root root 0 Feb 24 16:53 60\r\n-r-------- 1 root root 0 Feb 24 16:53 600\r\n-r-------- 1 root root 0 Feb 24 16:53 601\r\n-r-------- 1 root root 0 Feb 24 16:53 602\r\n-r-------- 1 root root 0 Feb 24 16:53 603\r\n-r-------- 1 root root 0 Feb 24 16:53 604\r\n-r-------- 1 root root 0 Feb 24 16:53 605\r\n-r-------- 1 root root 0 Feb 24 16:53 606\r\n-r-------- 1 root root 0 Feb 24 16:53 607\r\n-r-------- 1 root root 0 Feb 24 16:53 608\r\n-r-------- 1 root root 0 Feb 24 16:53 609\r\n-r-------- 1 root root 0 Feb 24 16:53 61\r\n-r-------- 1 root root 0 Feb 24 16:53 610\r\n-r-------- 1 root root 0 Feb 24 16:53 611\r\n-r-------- 1 root root 0 Feb 24 16:53 612\r\n-r-------- 1 root root 0 Feb 24 16:53 613\r\n-r-------- 1 root root 0 Feb 24 16:53 614\r\n-r-------- 1 root root 0 Feb 24 16:53 615\r\n-r-------- 1 root root 0 Feb 24 16:53 616\r\n-r-------- 1 root root 0 Feb 24 16:53 617\r\n-r-------- 1 root root 0 Feb 24 16:53 618\r\n-r-------- 1 root root 0 Feb 24 16:53 619\r\n-r-------- 1 root root 0 Feb 24 16:53 62\r\n-r-------- 1 root root 0 Feb 24 16:53 620\r\n-r-------- 1 root root 0 Feb 24 16:53 621\r\n-r-------- 1 root root 0 Feb 24 16:53 622\r\n-r-------- 1 root root 0 Feb 24 16:53 623\r\n-r-------- 1 root root 0 Feb 24 16:53 624\r\n-r-------- 1 root root 0 Feb 24 16:53 625\r\n-r-------- 1 root root 0 Feb 24 16:53 626\r\n-r-------- 1 root root 0 Feb 24 16:53 627\r\n-r-------- 1 root root 0 Feb 24 16:53 628\r\n-r-------- 1 root root 0 Feb 24 16:53 629\r\n-r-------- 1 root root 0 Feb 24 16:53 63\r\n-r-------- 1 root root 0 Feb 24 16:53 630\r\n-r-------- 1 root root 0 Feb 24 16:53 631\r\n-r-------- 1 root root 0 Feb 24 16:53 632\r\n-r-------- 1 root root 0 Feb 24 16:53 633\r\n-r-------- 1 root root 0 Feb 24 16:53 634\r\n-r-------- 1 root root 0 Feb 24 16:53 635\r\n-r-------- 1 root root 0 Feb 24 16:53 636\r\n-r-------- 1 root root 0 Feb 24 16:53 637\r\n-r-------- 1 root root 0 Feb 24 16:53 638\r\n-r-------- 1 root root 0 Feb 24 16:53 639\r\n-r-------- 1 root root 0 Feb 24 16:53 64\r\n-r-------- 1 root root 0 Feb 24 16:53 640\r\n-r-------- 1 root root 0 Feb 24 16:53 641\r\n-r-------- 1 root root 0 Feb 24 16:53 642\r\n-r-------- 1 root root 0 Feb 24 16:53 643\r\n-r-------- 1 root root 0 Feb 24 16:53 644\r\n-r-------- 1 root root 0 Feb 24 16:53 645\r\n-r-------- 1 root root 0 Feb 24 16:53 646\r\n-r-------- 1 root root 0 Feb 24 16:53 647\r\n-r-------- 1 root root 0 Feb 24 16:53 648\r\n-r-------- 1 root root 0 Feb 24 16:53 649\r\n-r-------- 1 root root 0 Feb 24 16:53 65\r\n-r-------- 1 root root 0 Feb 24 16:53 650\r\n-r-------- 1 root root 0 Feb 24 16:53 651\r\n-r-------- 1 root root 0 Feb 24 16:53 652\r\n-r-------- 1 root root 0 Feb 24 16:53 653\r\n-r-------- 1 root root 0 Feb 24 16:53 654\r\n-r-------- 1 root root 0 Feb 24 16:53 655\r\n-r-------- 1 root root 0 Feb 24 16:53 656\r\n-r-------- 1 root root 0 Feb 24 16:53 657\r\n-r-------- 1 root root 0 Feb 24 16:53 658\r\n-r-------- 1 root root 0 Feb 24 16:53 659\r\n-r-------- 1 root root 0 Feb 24 16:53 66\r\n-r-------- 1 root root 0 Feb 24 16:53 660\r\n-r-------- 1 root root 0 Feb 24 16:53 661\r\n-r-------- 1 root root 0 Feb 24 16:53 662\r\n-r-------- 1 root root 0 Feb 24 16:53 663\r\n-r-------- 1 root root 0 Feb 24 16:53 664\r\n-r-------- 1 root root 0 Feb 24 16:53 665\r\n-r-------- 1 root root 0 Feb 24 16:53 666\r\n-r-------- 1 root root 0 Feb 24 16:53 667\r\n-r-------- 1 root root 0 Feb 24 16:53 668\r\n-r-------- 1 root root 0 Feb 24 16:53 669\r\n-r-------- 1 root root 0 Feb 24 16:53 67\r\n-r-------- 1 root root 0 Feb 24 16:53 670\r\n-r-------- 1 root root 0 Feb 24 16:53 671\r\n-r-------- 1 root root 0 Feb 24 16:53 672\r\n-r-------- 1 root root 0 Feb 24 16:53 673\r\n-r-------- 1 ro^C\r\n'
